# Technical Spec: Metric-Specific LoRA Report Generation

## 1. Overview
Implement an automated reporting module that processes user scoring data (`scores.csv`) into a professional "LoRA Usage Guideline" (Markdown). The system must perform **independent analysis for each metric** defined in the CSV and use an LLM to generate the narrative.

## 2. Input Data Schema (`scores.csv`)
The system must parse the following columns (based on actual data samples):
* `metric`: String (e.g., "not ugly", "style fidelity"). **Crucial: Analysis must be grouped by this.**
* `weight`: Float (e.g., 0.5, 1.0).
* `prompt`: String.
* `choice`: String ("A", "B", "same").
* `stars`: Float/Int (Nullable).
* `lora_better`: String/Boolean ("True", "False", or Empty/NaN).

## 3. Data Preprocessing (The "Cleaning" Logic)
Before analysis, transform the raw CSV data into a standardized format.

### Normalization Logic table
Create valid calculation columns (`is_win`, `calc_score`) based on the user's inputs.

| Raw `lora_better` | Raw `choice` | Raw `stars` | **Derived `is_win`** | **Derived `calc_score`** | Logic Explanation |
| :--- | :--- | :--- | :--- | :--- | :--- |
| `"True"` / `True` | (Any) | `5` | **True** | **5.0** | LoRA won significantly. |
| `"True"` / `True` | (Any) | `NaN` / None | **True** | **3.0** | LoRA won, default to medium intensity (3.0). |
| `"False"` / `False`| "B" or "A" | (Any) | **False** | **0.0** | LoRA lost (Baseline was better). |
| `NaN` / Empty | `"same"` | (Any) | **False** | **0.0** | Tie / Neutral. Treat as "LoRA did not improve". |

**Implementation Note:**
* The analysis functions must loop through `df['metric'].unique()` to generate separate statistics for each metric.

## 4. Analysis Algorithms

Implement `generate_analysis_stats(df)` in `analysis.py`.

### Algorithm A: Metric-Specific Analysis (Loop over unique metrics)
For each `metric_name` in `df['metric'].unique()`:

1.  **Win Rate Analysis:**
    * Group by `weight`.
    * Formula: `Mean(is_win)` (Percentage of images where LoRA was better).
2.  **Quality Analysis:**
    * Group by `weight`.
    * Formula: `Mean(calc_score)` (Average star rating, treating losses/ties as 0).
3.  **Optimal Weight Discovery:**
    * Find the weight with the **highest Win Rate**.
    * *Tie-breaker:* If multiple weights have the same Win Rate, pick the one with the higher Average `calc_score`.
4.  **Prompt Compatibility:**
    * Filter data for this metric.
    * Group by `prompt`.
    * Identify **Top 2 prompts** (highest avg `calc_score`) and **Bottom 2 prompts**.

### Algorithm B: Visual Evidence Selector
Select representative images for the report:
* **Success Case:** Find a row where `is_win == True` AND `stars >= 4`. Return `lora_path`.
* **Failure Case:** Find a row where `is_win == False` (and `choice != 'same'` if possible). Return `lora_path` (to show what went wrong) or `baseline_path`.

## 5. LLM Integration (`utils.py`)

### JSON Payload Structure
Construct a dictionary that lists analysis per metric.
```json
{
  "lora_name": "My_Test_LoRA",
  "metrics_analysis": {
    "not ugly": {
      "best_weight": 0.5,
      "win_rate_at_best": 0.8,
      "best_prompts": ["A fluffy kitten..."],
      "worst_prompts": ["A sleek black cat..."]
    },
    "style consistency": {
      "best_weight": 0.8,
      "win_rate_at_best": 0.6
    }
  }
}

```

System Prompt

```

You are an expert AI Model Evaluator.
Write a multi-dimensional performance report for a LoRA model based on the provided statistics.

Structure:
1. **Executive Summary**: Overall verdict on the LoRA's utility.
2. **Metric-by-Metric Analysis**:
   - Create a subsection for EACH metric (e.g., "### Metric: Not Ugly").
   - For each metric, state the Optimal Weight and Win Rate.
   - Discuss which prompts worked best for that specific metric.
3. **Conclusion**: Final recommendation on how to use this LoRA.

Format: Markdown.
```

## 6. Frontend Visualization (app.py)
In the Report Tab:

    1. Iterate through metrics:

    - Display a sub-header: st.subheader(f"Analysis: {metric_name}").

    - Draw a Line Chart: X=weight, Y=Win Rate for that specific metric.

    2. LLM Report: Display the full markdown report generated by the LLM.

    3. Gallery: Display "Success Case" and "Failure Case" images below the text (if available).